{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "***\n",
    "**Context:** In this project, we'll learn to identify lane lines on the road.  We'll practice first on a single image, and later apply our tools to a video stream (really just a series of images).  Follow the steps below to complete the exercise.  Each step will be preceded by some \"Context\", and occasionally followed by a \"Note\" to the user.  \n",
    "\n",
    "**Step 1:** Let's have a look at our first image called 'test.jpg'.  Run the next 2 cells below (hit Shift-Enter or the \"play\" button above) to bring up an interactive matplotlib window displaying the image.\n",
    "\n",
    "**Note:** you can run each cell by first selecting it, then hitting \"Shift-Enter\" or pressing the play button in the toolbar above.  If, at any point, you encounter frozen display windows or other confounding issues, you can always start again with a clean slate by going to the \"Kernel\" menu above and selecting \"Restart & Clear Output\".\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This image is: ', <type 'numpy.ndarray'>, 'with dimesions:', (540, 960, 3))\n"
     ]
    }
   ],
   "source": [
    "image = mpimg.imread('test.jpg')\n",
    "print('This image is: ',type(image), 'with dimesions:', image.shape)\n",
    "plt.imshow(image)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**Context:** This is an image from a front facing camera on a car driving down the highway.  Have a look at the image. With our eyes, we can easily identify the lane lines right?\n",
    "\n",
    "**Step 2:**  So let's brainstorm ways we might be able to identify them with an algorithm... what kind of features do you think we could use to identify the lane lines in the image? Write your ideas in the cell below:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click here to enter your ideas:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** To start with, we're going to use color to identify the lane lines.  In the lesson we did this by hand, but here we're going to use an OpenCV function called \"inRange()\".   \n",
    "\n",
    "We have our image stored in the variable called 'image', which is an array with shape = (540, 960, 3).  Thus, our image is 540 pixels high, 960 pixels wide, and contains 3 color layers, one each for Red, Green, and Blue (RGB).  \n",
    "\n",
    "This is an 8 bit color image, meaning that values for [R,G,B] range from 0 to 255.  Pure white in an 8 bit image has [R,G,B] values of [255, 255, 255]. Our lane lines in the image aren't pure white, but they're close.  By implementing lower threshold (i.e., requiring RGB to be greater than some value) in [R,G,B] space, we can isolate pixels that are \"whiteish\".  \n",
    "\n",
    "\n",
    "**Step 3:** Use the interactive matplotlib window to explore the colors in the image.  When you move your cursor over the image, you will see the values for the x and y position of the cursor, as well as the three value color vector displayed at the bottom of the window.  For example: x=885.952, y=118.532 [134,194,231], implies that at position [x, y] = [885.952, 118.532], the color values [Red, Green, Blue] = [134, 194, 231].  \n",
    "\n",
    "Use the zoom tool (magnifying glass icon) in the matplotlib window toolbar to zoom in on the lane lines and explore their color.   Determine an appropriate lower threshold in [R,G,B] space for identifying pixels that are the same color as the lines.  \n",
    "\n",
    "In the cell below, replace the values in the variable \"lowWhiteThreshold\"  with your threshold values for [R,G,B].  We will then use the *cv2.inRange()* function to identify pixels that meet our color criteria.  This function returns a binary image with just one layer of color information, where all pixels meeting the criteria have been set to 255, and all others to 0. \n",
    "\n",
    "Run the cell below and try to set a threshold that retains the maximum number of pixels that correspond to lines on the road, without including a whole lot of pixels that are not the lines. Tweak your \"lowWhiteThreshold\" values until you get a result that looks approximately like this example image below. \n",
    "\n",
    "**Note:** hit the \"home\" icon in the matplotlib window to refresh and see the full image. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def colorSelect(image, white_low, white_high):\n",
    "    lower_white = np.array(white_low, dtype=np.uint8) \n",
    "    upper_white = np.array(white_high, dtype=np.uint8) \n",
    "    selected = cv2.inRange(image, lower_white, upper_white)\n",
    "    return selected\n",
    "\n",
    "lowWhiteThreshold = [1, 1, 1]\n",
    "highWhiteThreshold = [255, 255, 255]\n",
    "cselect = colorSelect(image, lowWhiteThreshold, highWhiteThreshold)\n",
    "#cv2.imwrite('color_selected.jpg', cselect)\n",
    "plt.imshow(cselect, cmap='Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"color_selected.jpg\" width=\"480\" alt=\"Color Selected Image\" />\n",
    " <p/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look approximately like this after color selection </p> \n",
    " </figcaption>\n",
    "</figure>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Context:** We have now drastically simplified our image to contain only binary values (0 or 255), where zeros correspond to pixels that did not meet our color criterion.  \n",
    "\n",
    "We are now ready to detect lines in the image using a \"Hough Transform\". A Hough transform can be used to detect various shapes in an image, but here we'll\n",
    "use it to detect lines.  Check out the <A HREF=\"http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html#HoughLinesP\" target=\"_blank\"> OpenCV docs on feature detection</A> and scroll down to the HoughLinesP() function learn more.\n",
    "\n",
    "**Step 4:** Here we'll write another couple functions to perform the Hough transform and then draw the detected lines back onto our original image. The Hough Transform takes several parameters (rho, theta, threshold, minLineLength, and maxLineGap).  Read the docs in the links above to find out what these parameters do, and play around with different values until your output image looks roughly like the sample image below.\n",
    "\n",
    "**Note:** cv2.HoughLinesP() returns \"lines\", which is an array containing the endpoints of all the line segments detected.  So, for example, lines[0] will be a four element array [x1, y1, x2, y2], where (x1, y1) and (x2, y2) are the endpoints of the first detected line segment.\n",
    "\n",
    "After detecting lines, we define a function \"drawLane()\" to draw each of the detected line segments onto our image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def houghLinesP(imageCopy, cselected, rho, theta, threshold, minLineLength, maxLineGap):\n",
    "    lines = cv2.HoughLinesP(cselected, rho, theta, threshold, np.array([]),\n",
    "                            minLineLength, maxLineGap)\n",
    "    drawLane(imageCopy, lines)\n",
    "    return lines, imageCopy\n",
    "\n",
    "def drawLane(image, lines):\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                 cv2.line(image,(x1,y1),(x2,y2),(200,0,0),2)\n",
    "    return\n",
    "\n",
    "#we'll actually draw the lines onto a blank image, then coadd it to the original below\n",
    "def blank(image):\n",
    "    return np.zeros(image.shape, np.uint8)\n",
    "\n",
    "#the Hough transform takes various parameters\n",
    "#see the links above to learn more about what they do.\n",
    "rho = 1\n",
    "theta = np.pi/180\n",
    "threshold = 1\n",
    "minLineLength = 10\n",
    "maxLineGap = 3\n",
    "\n",
    "lines, lineImage = houghLinesP(blank(image), cselect, rho, theta, threshold, minLineLength, maxLineGap)\n",
    "combo = cv2.addWeighted(image, 0.8, lineImage, 1, 0)\n",
    "\n",
    "#cv2.imwrite('laneLines_firstPass.jpg', cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(combo)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"laneLines_firstPass.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <p/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look approximately like this after line detection </p> \n",
    " </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** At this point, we have detected many lines in our color selected image.  Clearly, some of them correspond to our lane lines, and others correspond to junk we don't want.  To get rid of the junk, we will define a \"region of interest\" in the image, such that, going forward, we will only try to detect lines in this region.\n",
    "\n",
    " In this step, we want to isolate the region in the image where as many of the line segments that we want are, and eliminate line segments that we don't want.  While we could make it any shape we want, we will start by defining a triangular region that basically includes our driving lane and nothing else.\n",
    " \n",
    "**Step 5:** Choose values for the vertices of a triangle (modify the array called \"vertices\" below) that encompasses our region of interest in the image. Run the cell below and examine the output.  Your output image should look like the masked image shown below.\n",
    "\n",
    "**Note:** The y-axis in the image is flipped, so the values run from 0 at the top of the frame to 539 at the bottom.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def regionOfInterest(img, verts):\n",
    "    \n",
    "    mask = blank(img)\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,)*channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    cv2.fillPoly(mask, verts, ignore_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "vertices = np.array([[(0,539), (959,400), (440,0)]], dtype=np.int32)\n",
    "masked = regionOfInterest(image, vertices)\n",
    "#cv2.imwrite('masked.jpg', cv2.cvtColor(masked, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "plt.imshow(masked)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"masked.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <p/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look approximately like this after masking </p> \n",
    " </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** Now we have identified our region of interest.  We will isolate this region of interest in our color selected image from above, and rerun our line detection.\n",
    " \n",
    "**Step 6:** Run the cell below to create a masked version of the color selected image and rerun the line detection step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked = regionOfInterest(cselect, vertices)\n",
    "lines, lineImage = houghLinesP(blank(image), masked, rho, theta, threshold, minLineLength, maxLineGap)\n",
    "combo = cv2.addWeighted(image, 0.75, lineImage, 1, 0)\n",
    "#cv2.imwrite('laneLines_secondPass.jpg', cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(combo)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"laneLines_secondPass.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <p/>\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look approximately like this after line detection </p> \n",
    " </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** Now we've got our lane lines identified, and no extraneous junk to worry about. We'd now like to draw an estimate of where the lane is on the image, based on the detected line segments.  To do this, we will fit a line to to all the detected line segments, and extrapolate up and down the road. \n",
    "\n",
    "At this point, we're also going to take into account the slope of the detected line segments, taking advantage of the fact that we know our line detections for the righthand lane line should all have positive slope, and those for the line on the left have negative slope (recall that the y-axis is flipped).\n",
    " \n",
    "**Step 7:** Run the cell below to redefine the drawLane function, now to extrapolate the detected line segments up and down the road.  This will output the original image with our full lane lines drawn on.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def houghLinesPExtra(imageCopy, cselected, rho, theta, threshold, minLineLength, maxLineGap):\n",
    "    lines = cv2.HoughLinesP(cselected, rho, theta, threshold, np.array([]),\n",
    "                            minLineLength, maxLineGap)\n",
    "    drawLaneExtra(imageCopy, lines)\n",
    "    return lines, imageCopy\n",
    "\n",
    "\n",
    "def drawLaneExtra(image, lines):\n",
    "    \n",
    "    midpointx = vertices[0][2][0]\n",
    "    rightX = []\n",
    "    leftX = []\n",
    "    rightY = []\n",
    "    leftY = []\n",
    "    slopeCut = 0.6  \n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            for x1,y1,x2,y2 in line:\n",
    "                rise = float(y2 - y1)\n",
    "                run = float(x2 - x1)\n",
    "                if run != 0:\n",
    "                    slope = rise/run  \n",
    "                    if slope > slopeCut and x1 > (midpointx-20) and x2 > (midpointx-20): \n",
    "                        #identifying lane line segments on the right side of the frame\n",
    "                        rightX.extend([x1, x2])\n",
    "                        rightY.extend([y1, y2])\n",
    "                    elif slope < -slopeCut and x1 < (midpointx+20) and x2 < (midpointx+20): \n",
    "                        #identifying lane line segments on the left side of the frame\n",
    "                        leftX.extend([x1, x2])\n",
    "                        leftY.extend([y1, y2])                   \n",
    "\n",
    "        if leftX and leftY:\n",
    "            fitLeft = np.polyfit(leftX, leftY, 1)\n",
    "            startYleft = 0\n",
    "            endYleft = image.shape[0]\n",
    "            startXleft = int((startYleft - fitLeft[1]) / fitLeft[0])\n",
    "            endXleft = int((endYleft - fitLeft[1]) / fitLeft[0])\n",
    "            cv2.line(image,(startXleft,startYleft),(endXleft,endYleft),(255,0,0),10)\n",
    "            \n",
    "        if rightX and rightY:\n",
    "            fitRight = np.polyfit(rightX, rightY, 1)\n",
    "            startYright = 0\n",
    "            endYright = image.shape[0]\n",
    "            startXright = int((startYright - fitRight[1]) / fitRight[0])\n",
    "            endXright = int((endYright - fitRight[1]) / fitRight[0])\n",
    "            cv2.line(image,(startXright,startYright),(endXright,endYright),(255,0,0),10)\n",
    "        \n",
    "lines, lineImage = houghLinesPExtra(blank(image), masked, rho, theta, \n",
    "                                    threshold, minLineLength, maxLineGap)\n",
    "lineMasked = regionOfInterest(lineImage, vertices)\n",
    "combo = cv2.addWeighted(image, 0.75, lineMasked, 1, 0)\n",
    "#cv2.imwrite('laneLines_thirdPass.jpg', cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(combo)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"laneLines_thirdPass.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look approximately like this after line extrapolation </p> \n",
    " </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** We've done it!  We now have a pretty good estimate of where the lane is.  \n",
    " \n",
    "**Step 8:** In the cell below, explain in your own words what steps we took to get to this point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click here to enter your explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** We are now ready to run our code on a video stream.\n",
    " \n",
    "**Step 9:** Run the cell below to see how our algorithm does on a stream of images.\n",
    "\n",
    "**Note:** The cv2 display window may pop up behind your browser.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.startWindowThread()\n",
    "cv2.namedWindow('Lane-Finding')\n",
    "cap = cv2.VideoCapture('Highway_Driving.mp4')\n",
    "count = 0\n",
    "while cap.isOpened():\n",
    "    count += 1\n",
    "    ret, image = cap.read()\n",
    "    if image is not None:\n",
    "        cselect = colorSelect(image, lowWhiteThreshold, highWhiteThreshold)\n",
    "        masked = regionOfInterest(cselect, vertices)\n",
    "        lines, lineImage = houghLinesPExtra(blank(image), masked, rho, theta, \n",
    "                                            threshold, minLineLength, maxLineGap)\n",
    "        lineMasked = regionOfInterest(lineImage, vertices)\n",
    "        combo = cv2.addWeighted(image, 0.75, cv2.cvtColor(lineMasked, cv2.COLOR_BGR2RGB), 1, 0)\n",
    "        cv2.imshow('Lane-Finding',combo)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27 :\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "**Context:** Fantastic!  It works (more or less)!  Things to think about going forward are: Where is this going to fail?  How can we make our algorithm more robust? \n",
    " \n",
    "**Step 10:** In the cell below, write down your thoughts on possible failure modes and how to improve our algorithm:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click here to enter your ideas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Context:** Let's test a new scenario and see how our algorithm performs.\n",
    "<p></p> \n",
    "<p></p> \n",
    "<figure>\n",
    " <img src=\"test2.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> What will we find using this new image? </p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "**Step 11:** Run the cell below to test our algorithm on a new image.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread('test2.jpg')\n",
    "\n",
    "cselect = colorSelect(image, lowWhiteThreshold, highWhiteThreshold)\n",
    "vertices = vertices #might want to change this!\n",
    "masked = regionOfInterest(image, vertices)\n",
    "\n",
    "plt.imshow(masked)\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked = regionOfInterest(cselect, vertices)\n",
    "lines, lineImage = houghLinesPExtra(blank(image), masked, rho, theta, \n",
    "                                    threshold, minLineLength, maxLineGap)\n",
    "lineMasked = regionOfInterest(lineImage, vertices)\n",
    "combo = cv2.addWeighted(image, 0.75, lineMasked, 1, 0)\n",
    "plt.imshow(combo)#, cmap = 'Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**Context:** It seems we have a problem here!  What do you think is going wrong?\n",
    "\n",
    "**Step 12:** In the cell below, write down your thoughts on what the problem is and how to accommodate this issue in our algorithm:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double click here to enter your ideas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** As we well know, not all lane lines are white, so we'll have to take this into account.  At this point, we'll shift away from a color selection, and try to improve our detection of the lines solely based on shape.\n",
    "\n",
    "In this next series of cells, we will perform edge detection on our input image.  First, however, we need to convert to grayscale (i.e. go from 3 dimensions in color to just 1).  \n",
    "\n",
    "**Step 12:** Run the cell below to convert our input image to grayscale and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Grayscale image is: ', <type 'numpy.ndarray'>, 'with dimesions:', (540, 960))\n"
     ]
    }
   ],
   "source": [
    "gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "print('Grayscale image is: ',type(gray), 'with dimesions:', gray.shape)\n",
    "#cv2.imwrite('grayScale.jpg', gray)\n",
    "plt.imshow(gray, cmap='Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"grayScale.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look like this after grayscaling </p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** We can see from the printout that our grayscale image is two dimensional, reduced from three dimensions in the original color image.\n",
    "\n",
    "We see a problem however, in that the yellow line on the left has been converted into a medium shade of gray, which will be hard to detect against the background.\n",
    "\n",
    "At this point, we back up a step and try a color transform known as \"Hue, Light, and Saturation\" (HLS) before grayscaling the image.  This conversion is commonly used to bring out the brighter colors in an image.  After the conversion we'll grayscale by simply summing along the color axis.\n",
    "\n",
    "**Step 12:** Run the cell below to convert our input image to grayscale via HLS color transform and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grayViaHLS(img):\n",
    "    hls = cv2.cvtColor(img,cv2.COLOR_BGR2HLS)\n",
    "    hlsGray = np.uint8(np.sum(hls, axis=2)/3)\n",
    "    return hlsGray\n",
    "\n",
    "hlsGray = grayViaHLS(image)\n",
    "#cv2.imwrite('hlsGray.jpg', hlsGray)\n",
    "plt.imshow(hlsGray, cmap='Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"hlsGray.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look like this after grayscaling via HLS </p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** Great!  We've got our yellow line back and now we're ready to detect edges.  To do this we'll use a Canny edge detector (an algorithm developed by John F. Canny in 1986).\n",
    "\n",
    "The basic premise behind the Canny edge detection algorithm is that we're going to look for rapid changes in brightness from pixel to pixel across our image.  You'll first smooth the image with a Gaussian kernel to get rid of noise fluctuations that might otherwise look like edges to the Canny algorithm.  You'll then measure the derivative (change) of intensity from pixel to pixel, both vertically and horizontally, at all points in the image.  Then play with the thresholds to appropriately to pick out the edges you want.  Check out <A HREF=\"http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html\" target=\"_blank\"> the OpenCV docs on Canny edge detection</A> to learn more\n",
    "\n",
    "**Step 13:** Run the cell below and play with the parameters (kernelSize, lowerThresh, upperThresh, and apSize) to perform Canny edge detection on our grayscaled image.  The output will be a binary image showing all pixels associated with edges in white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernelSize = 5\n",
    "blurGray = cv2.GaussianBlur(hlsGray,(kernelSize, kernelSize),0)\n",
    "\n",
    "lowerThresh = 50\n",
    "upperThresh = 150\n",
    "apSize = 3\n",
    "edges = cv2.Canny(blurGray, lowerThresh, upperThresh, apertureSize = apSize)\n",
    "#cv2.imwrite('edges.jpg', edges)\n",
    "plt.imshow(edges, cmap='Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"edges.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look like this after edge detection </p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Context:** Ok, now we've got our edges detected and we can run our Hough transform on the output of the Canny algorithm to detect lines.  First, we'll crop out our region of interest as before.\n",
    "\n",
    "**Step 13:** Run the cell below to crop the edge detected image and run a Hough transform on it to detect lines.  We'll then plot up the detected lines on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked = regionOfInterest(edges, vertices)\n",
    "lines, lineImage = houghLinesPExtra(blank(image), masked, rho, theta, \n",
    "                                    threshold, minLineLength, maxLineGap)\n",
    "lineMasked = regionOfInterest(lineImage, vertices)\n",
    "combo = cv2.addWeighted(image, 0.75, lineMasked, 1, 0)\n",
    "#cv2.imwrite('laneLines_fourthPass.jpg', cv2.cvtColor(combo, cv2.COLOR_BGR2RGB))\n",
    "plt.imshow(combo)#, cmap = 'Greys_r')\n",
    "fig = plt.gcf()\n",
    "fig.canvas.manager.window.raise_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    " <img src=\"laneLines_fourthPass.jpg\" width=\"480\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Your output image should look like this after line detection </p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context:** Nailed it!  Awesome!  Now lets try another video stream\n",
    "\n",
    "**Step 13:** Run the cell below to apply our new algorithm to a video stream.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.startWindowThread()\n",
    "cv2.namedWindow('Lane-Finding')\n",
    "cap = cv2.VideoCapture('yellowLeft.mp4')\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, image = cap.read()\n",
    "    if image is not None:\n",
    "        hlsGray = grayViaHLS(image)\n",
    "        blurGray = cv2.GaussianBlur(hlsGray,(kernelSize, kernelSize), 0)\n",
    "        edges = cv2.Canny(blurGray, lowerThresh, upperThresh, apertureSize = apSize)\n",
    "        masked = regionOfInterest(edges, vertices)\n",
    "        lines, lineImage = houghLinesPExtra(blank(image), masked, rho, theta, \n",
    "                                            threshold, minLineLength, maxLineGap)\n",
    "        lineMasked = regionOfInterest(lineImage, vertices)\n",
    "        combo = cv2.addWeighted(image, 0.75, cv2.cvtColor(lineMasked, cv2.COLOR_BGR2RGB), 1, 0)\n",
    "        cv2.imshow('Lane-Finding',combo)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27 :\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
